{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <img src=\"https://www.verifia.ca/assets/logo.png\" width=\"160px\" alt=\"VerifIA Logo\"/><br>\n",
    "  <strong>© 2025 VerifIA. All rights reserved.</strong>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VerifIA - Model Verification: Loan Eligibility Prediction with XGBoost\n",
    "\n",
    "This notebook addresses a loan eligibility prediction problem using an XGBoost classifier. We tune the model via Bayesian hyperparameter optimization (using BayesSearchCV) with cross-validation. After selecting the best parameters, the trained model is wrapped with VerifIA’s XGBModel wrapper. Next, we generate a domain configuration—either automatically using AI and external domain documents or by loading a pre-defined YAML file—and then verify the model’s rule consistency using VerifIA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install requests\n",
    "!pip install scikit-optimize\n",
    "!pip install \"../../dist/verifia-0.1.0-py3-none-any.whl[xgboost, genflow]\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 0. Download Resources\n",
    "\n",
    "Before running any other cells, make sure you have all required resources:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -sL https://tinyurl.com/r6m2zk87 -o downloader.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'extracted': True,\n",
       " 'files': ['articles/',\n",
       "  'articles/Criteria You Need to Know Before You Apply.pdf',\n",
       "  'articles/Personal loan elegibility.pdf',\n",
       "  'articles/Personal loan requirements.pdf',\n",
       "  'articles/What Are Personal Loan Eligibility Requirements.pdf',\n",
       "  'data_report.pdf',\n",
       "  'domain_definition_meeting_notes.pdf',\n",
       "  'domain_definition_report.pdf',\n",
       "  'feature_selection.pdf',\n",
       "  'sensitivity_analysis_meeting_notes.pdf',\n",
       "  'sensitivity_analysis_report.pdf']}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from downloader import download_resource\n",
    "url = 'https://www.verifia.ca/assets/use-cases/'\n",
    "download_resource(url+'data/loan_eligibility.csv', \n",
    "                  dest_dir='../data')\n",
    "download_resource(url+'domains/loan_eligibility.yaml', \n",
    "                  dest_dir='../domains')\n",
    "download_resource(url+'documents/loan_eligibility.zip', \n",
    "                  dest_dir='../documents/loan_eligibility')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Importing Libraries and Setting Up\n",
    "\n",
    "We begin by importing required libraries such as Pandas, NumPy, XGBoost, and modules from skopt and VerifIA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "import getpass\n",
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.callbacks import DeadlineStopper, DeltaYStopper\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from verifia.models import XGBModel, build_from_model_card\n",
    "from verifia.verification.results import RulesViolationResult\n",
    "from verifia.context.data import Dataset\n",
    "from verifia.verification.verifiers import RuleConsistencyVerifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Data Loading\n",
    "\n",
    "Constants are defined (e.g., random seed, model directory paths, and data file path). The loan eligibility dataset is loaded from a CSV file. The target variable (*loan_paid*) is separated from the feature columns, and categorical features are identified from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "RAND_SEED = 0\n",
    "MODELS_DIRPATH = \"../models\"\n",
    "DATA_PATH = \"../data/loan_eligibility.csv\"\n",
    "dataframe = pd.read_csv(DATA_PATH)\n",
    "target_name = \"loan_paid\"\n",
    "feature_names = set(dataframe.columns) - {target_name}\n",
    "cat_feature_names = set(dataframe.select_dtypes(include=[\"object\"]).columns) - {target_name}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Building the XGBoost Model Wrapper\n",
    "\n",
    "Using VerifIA’s `build_from_model_card`, we create an instance of `XGBModel`. This wrapper stores critical metadata (such as model name, version, type, feature names, categorical feature names, target name, and the local directory for model storage) and provides a standardized interface for verification tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_wrapper:XGBModel = build_from_model_card({\n",
    "    \"name\": \"loan_eligibility\",\n",
    "    \"version\": \"2\",\n",
    "    \"type\": \"classification\",\n",
    "    \"description\": \"model predicts the loan eligibility of a customer.\",\n",
    "    \"framework\": \"xgboost\",\n",
    "    \"feature_names\": feature_names,\n",
    "    \"cat_feature_names\": cat_feature_names,\n",
    "    \"target_name\": target_name,\n",
    "    \"local_dirpath\": MODELS_DIRPATH\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Preparing the Dataset\n",
    "\n",
    "The loaded DataFrame is transformed into a VerifIA `Dataset` object. This object organizes the feature and target data, automatically detecting categorical features. The dataset is then split into training and testing subsets (using an 80/20 split) to facilitate model tuning and subsequent evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset(dataframe, model_wrapper.target_name, \n",
    "                  model_wrapper.feature_names, \n",
    "                  model_wrapper.cat_feature_names)\n",
    "train_dataset, test_dataset = dataset.split(0.8, RAND_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Defining and Tuning the XGBoost Classifier\n",
    "\n",
    "An XGBoost classifier is initialized with categorical feature support and a fixed random seed.  \n",
    "A search space for key hyperparameters (including number of estimators, learning rate, maximum tree depth, minimum child weight, gamma, subsample ratio, column sample by tree, and regularization terms) is defined. Using BayesSearchCV with StratifiedKFold cross-validation, we perform Bayesian hyperparameter tuning. Callback functions such as a time-based stopper and an evaluation step printer monitor the optimization progress. The best hyperparameters are extracted after the search.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model = xgb.XGBClassifier(enable_categorical=True, random_state=RAND_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_splits_count, max_trials, n_hparams_at_trial = 5, 10, 3\n",
    "skf = StratifiedKFold(n_splits=cv_splits_count, shuffle=True, random_state=RAND_SEED)\n",
    "search_spaces = {\n",
    "                'n_estimators': [10, 50, 100, 200, 400, 600, 800, 1000],\n",
    "                'learning_rate': [0.001, 0.01, 0.1, 0.2],\n",
    "                'max_depth': [3, 6, 9, 12],\n",
    "                'min_child_weight': [1, 3, 5, 7],\n",
    "                'gamma': [0.0, 0.1, 0.2, 0.3, 0.4],\n",
    "                'subsample': [0.5, 0.75, 1.0],\n",
    "                'colsample_bytree': [0.5, 0.75, 1.0],\n",
    "                'reg_alpha': [0.0, 0.1, 0.5, 1.0],\n",
    "                'reg_lambda': [0.0, 0.1, 0.5, 1.0]\n",
    "                }\n",
    "hparams_tuner = BayesSearchCV(estimator=xgb_model,                                    \n",
    "                    search_spaces=search_spaces,                      \n",
    "                    scoring='f1',                                  \n",
    "                    cv=skf,                               # number of splits for cross-validation            \n",
    "                    n_iter=max_trials,                                # max number of trials\n",
    "                    n_points=n_hparams_at_trial,                      # number of hyperparameter sets evaluated at the same time\n",
    "                    iid=False,                                        # if not iid it optimizes on the cv score\n",
    "                    return_train_score=False,                         \n",
    "                    refit=False,                                      \n",
    "                    optimizer_kwargs={'base_estimator': 'GP'},        # optmizer parameters: we use Gaussian Process (GP)\n",
    "                    n_jobs=-1,                                      \n",
    "                    random_state=RAND_SEED) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 1\n",
    "def onstep(res):\n",
    "    global counter\n",
    "    x0 = res.x_iters   # List of input points\n",
    "    y0 = res.func_vals # Evaluation of input points\n",
    "    print(f'Last eval #{counter}: {x0[-1]}', \n",
    "          f' - Score {y0[-1]:.3f}')\n",
    "    print(f' - Best Score {res.fun:.3f}',\n",
    "          f' - Best Args: {res.x}')\n",
    "    counter += 1\n",
    "\n",
    "#overdone_control = DeltaYStopper(delta=0.0001)               # We stop if the gain of the optimization becomes too small\n",
    "time_limit_control = DeadlineStopper(total_time=60 * 45)     # We impose a time limit (45 minutes)\n",
    "\n",
    "callbacks=[time_limit_control, onstep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last eval #1: [1.0, 0.2, 0.01, 9, 5, 100, 0.1, 0.5, 0.5]  - Score -0.911\n",
      " - Best Score -0.911  - Best Args: [1.0, 0.2, 0.01, 9, 5, 100, 0.1, 0.5, 0.5]\n",
      "Last eval #2: [1.0, 0.0, 0.1, 9, 1, 200, 1.0, 1.0, 0.5]  - Score -0.908\n",
      " - Best Score -0.911  - Best Args: [1.0, 0.2, 0.01, 9, 5, 100, 0.1, 0.5, 0.5]\n",
      "Last eval #3: [0.5, 0.1, 0.1, 9, 1, 100, 0.0, 1.0, 1.0]  - Score -0.911\n",
      " - Best Score -0.911  - Best Args: [0.5, 0.1, 0.01, 6, 1, 600, 0.5, 1.0, 0.75]\n",
      "Last eval #4: [0.5, 0.3, 0.01, 6, 7, 600, 1.0, 0.1, 0.75]  - Score -0.911\n",
      " - Best Score -0.911  - Best Args: [0.5, 0.1, 0.01, 6, 1, 600, 0.5, 1.0, 0.75]\n",
      "candidates checked: 10, best CV score: 0.911, best_score_std:0.001\n",
      "best_params: OrderedDict([('colsample_bytree', 0.5), ('gamma', 0.1), ('learning_rate', 0.01), ('max_depth', 6), ('min_child_weight', 1), ('n_estimators', 600), ('reg_alpha', 0.5), ('reg_lambda', 1.0), ('subsample', 0.75)])\n"
     ]
    }
   ],
   "source": [
    "X = train_dataset.feature_data(True)\n",
    "y = train_dataset.target_data\n",
    "hparams_tuner.fit(X, y, callback=callbacks)\n",
    "\n",
    "hparams_evals_count = len(hparams_tuner.cv_results_['params'])\n",
    "best_score = hparams_tuner.best_score_\n",
    "best_score_std = pd.DataFrame(hparams_tuner.cv_results_).iloc[hparams_tuner.best_index_].std_test_score\n",
    "best_params = hparams_tuner.best_params_\n",
    "print(f\"candidates checked: {hparams_evals_count}, best CV score: {best_score:.3f}, best_score_std:{best_score_std:.3f}\")\n",
    "print(f\"best_params: {best_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Training and Wrapping the Final XGBoost Model\n",
    "\n",
    "The XGBoost model is re-instantiated with the best hyperparameters from the tuning phase. The model is then trained on the training data, and the model is assigned to the XGBModel wrapper. We evaluate its performance on the test dataset using the f1 score (or any other relevant metric), ensuring that the model meets the desired classification performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Performance Metric : F1-Score=0.9110096549659457\n"
     ]
    }
   ],
   "source": [
    "xgb_model = xgb.XGBClassifier(**best_params, enable_categorical=True, random_state=RAND_SEED)\n",
    "xgb_model.fit(X, y)\n",
    "model_wrapper.wrap_model(xgb_model)\n",
    "model_wrapper.save_model()\n",
    "model_wrapper.load_model()\n",
    "metric_name, metric_score = model_wrapper.calculate_predictive_performance(test_dataset)\n",
    "print(f\"Test Performance Metric : {metric_name}={metric_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. Loading or Generating the Domain Configuration\n",
    "\n",
    "VerifIA allows you to create a domain configuration in two way. With the domain configuration available (either generated or loaded), we instantiate the `RuleConsistencyVerifier`. This verifier uses the domain rules and constraints to evaluate whether the model’s predictions on the test data are consistent with our domain knowledge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Option A: Predefined Domain File:**\n",
    "\n",
    "You can load a predefined domain YAML file (e.g., \"loan_eligibility.yaml\") to provide the necessary constraints and rules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "DOMAIN_PATH = f\"../domains/loan_eligibility.yaml\"\n",
    "model_verifier = RuleConsistencyVerifier(DOMAIN_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Option B: AI-Powered Domain Generation:**  \n",
    "\n",
    "Alternatively, VerifIA’s DomainGenFlow module is used to generate a rich domain configuration from the training data. By supplying the training dataframe, a directory of PDF documents containing domain knowledge, and the model card information, a domain configuration dictionary is produced."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Setup OpenAI and LangSmith Keys**\n",
    "\n",
    "Note that LangSmith is not needed, but it is helpful. If you do want to use LangSmith, after you sign up at the link above, make sure to set your environment variables to start logging traces.\n",
    "\n",
    "Accessing the OpenAI API requires an API key, which you can get by creating an account. Once you have a key you'll want to set it as an environment variable by running:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = 'true'\n",
    "os.environ[\"LANGCHAIN_ENDPOINT\"] = 'https://api.smith.langchain.com'\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = getpass.getpass(prompt='Your LANGCHAIN_API_KEY? ')\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(prompt='Your OPENAI_API_KEY? ')\n",
    "os.environ[\"USER_AGENT\"] = 'my_agent'\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = 'VERIFIA_TEST'\n",
    "os.environ[\"VERIFIA_GPT_NAME\"] = 'gpt-4.1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from verifia.generation import DomainGenFlow\n",
    "\n",
    "DOMAIN_PDF_DIRPATH = \"../documents/loan_eligibility\"\n",
    "domain_genflow = DomainGenFlow()\n",
    "domain_genflow.load_ctx(dataframe=train_dataset.data,\n",
    "                        pdfs_dirpath=DOMAIN_PDF_DIRPATH,\n",
    "                        model_card=model_wrapper.model_card.to_dict())\n",
    "domain_cfg_dict = domain_genflow.run(save=True, local_path=\"./domain.yaml\")\n",
    "model_verifier = RuleConsistencyVerifier(domain_cfg_dict=domain_cfg_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. Running the Rule Consistency Verifier\n",
    "\n",
    "Using the generated domain configuration (or the loaded YAML file), we instantiate a `RuleConsistencyVerifier`. This verifier is then connected to the wrapped XGBoost model and the test dataset. We run the verification process using a Random Sampler (RS) search algorithm with specified parameters (population size, maximum iterations, and original seed size). The verifier explores the input space to identify any rule violations or inconsistencies in the model’s predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Original Inputs: 10it [00:00, 17.08it/s]it/s]\n",
      "Processing Original Inputs: 10it [00:00, 15.06it/s]5,  1.69it/s]\n",
      "Processing Original Inputs: 10it [00:00, 13.96it/s]5,  1.56it/s]\n",
      "Processing Original Inputs: 10it [00:00, 18.18it/s]5,  1.47it/s]\n",
      "Processing Original Inputs: 10it [00:00, 16.26it/s]4,  1.59it/s]\n",
      "Processing Original Inputs: 10it [00:00, 17.70it/s]3,  1.60it/s]\n",
      "Processing Original Inputs: 10it [00:00, 16.86it/s]3,  1.65it/s]\n",
      "Processing Original Inputs: 10it [00:00, 13.31it/s]2,  1.66it/s]\n",
      "Processing Original Inputs: 10it [00:00, 15.19it/s]1,  1.53it/s]\n",
      "Processing Original Inputs: 10it [00:00, 14.40it/s]1,  1.53it/s]\n",
      "Processing Original Inputs: 10it [00:00, 16.48it/s]00,  1.50it/s]\n",
      "Processing Rules: 100%|██████████| 11/11 [00:07<00:00,  1.56it/s]\n"
     ]
    }
   ],
   "source": [
    "result:RulesViolationResult = model_verifier.verify(model_wrapper)\\\n",
    "                                            .on(test_dataset.data)\\\n",
    "                                            .using(\"RS\")\\\n",
    "                                            .run(pop_size=4, max_iters=5, orig_seed_size=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9. Saving the Verification Report and Model Artifacts\n",
    "\n",
    "Finally, the verification results are saved as an HTML report that provides a detailed summary of rule compliance and any detected violations. Additionally, the trained XGBoost model and its model card are saved, ensuring that the model’s configuration and verification status are archived for future reference or reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.save_as_html(\"../reports/loan_eligibility.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_wrapper.save_model()\n",
    "model_wrapper.save_model_card(\"../models/loan_eligibility.yaml\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
