{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <img src=\"https://www.verifia.ca/assets/logo.png\" width=\"160px\" alt=\"VerifIA Logo\"/><br>\n",
    "  <strong>© 2025 VerifIA. All rights reserved.</strong>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install \"../../dist/verifia-0.1.0-py3-none-any.whl[genflow]\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VerifIA - Rule-Based Verification Quickstart\n",
    "\n",
    "This notebook demonstrates two approaches for generating a domain configuration for model verification using the California housing dataset. You can either manually create a simple domain dictionary based on your intuition or use VerifIA’s AI-powered generation flow to create a domain file automatically from the dataframe and descriptive domain knowledge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Importing Libraries and Setting Up\n",
    "\n",
    "In this section, we import the required Python libraries and modules. These include standard data science package, scikit-learn, and the specific modules from the VerifIA tool that handle model wrapping and rule verification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdotenv\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_dotenv\n\u001b[0;32m      5\u001b[0m load_dotenv() \n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpipeline\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m make_pipeline\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m StandardScaler\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PolynomialFeatures\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "import getpass\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv() \n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import root_mean_squared_error, mean_absolute_percentage_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from verifia.models import SKLearnModel, build_from_model_card\n",
    "from verifia.verification.results import RulesViolationResult\n",
    "from verifia.verification.verifiers import RuleConsistencyVerifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Setting Constants and Directories\n",
    "\n",
    "We define some key constants such as the random seed for reproducibility and the directory path where our model artifacts will be saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "RAND_SEED = 0\n",
    "MODELS_DIRPATH = \"../models\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Loading the California Housing Data\n",
    "\n",
    "Using scikit-learn’s `fetch_california_housing`, we load the California housing dataset into a DataFrame. We extract the feature names and target name directly from the dataset, which will be used later for model training and verification. The dataset is split into training and testing subsets (80/20 split) to allow for model training on one set and evaluation/verification on the other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sklearn.datasets._california_housing:Downloading Cal. housing from https://ndownloader.figshare.com/files/5976036 to C:\\Users\\Ahmed\\scikit_learn_data\n"
     ]
    }
   ],
   "source": [
    "housing = fetch_california_housing(as_frame=True)\n",
    "housing_df = housing.frame\n",
    "feature_names = housing.feature_names\n",
    "target_name = housing.target_names[0]\n",
    "train_df, test_df = train_test_split(housing_df, train_size=0.8, random_state=RAND_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Training Multiple Regression Models\n",
    "\n",
    "We create and train several regression models using different pipelines:\n",
    "- **Polynomial Regression:** A pipeline with standard scaling, polynomial feature expansion (degree 2), and linear regression.\n",
    "- **Decision Tree Regressor:** A simple decision tree model with standard scaling.\n",
    "- **Random Forest Regressor:** An ensemble model using random forests.\n",
    "- **MLP Regressor:** A multi-layer perceptron with a specific hidden layer configuration.\n",
    "\n",
    "Each model is trained on the training data, and key performance metrics (RMSE and MAPE) are computed on the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5.1 Optional: Run this cell to train the polynomial regression model\n",
    "This cell trains a polynomial regression model using a 2nd degree polynomial expansion. The model is scaled with StandardScaler and fit using LinearRegression, which helps capture non-linear relationships in your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.6624819110715112 0.2864130701376177\n"
     ]
    }
   ],
   "source": [
    "poly_reg = make_pipeline(StandardScaler(), PolynomialFeatures(degree=2), LinearRegression())\n",
    "poly_reg.fit(train_df[feature_names], train_df[target_name])\n",
    "poly_predictions = poly_reg.predict(test_df[feature_names])\n",
    "poly_rmse = root_mean_squared_error(test_df[target_name], poly_predictions)\n",
    "poly_mape = mean_absolute_percentage_error(test_df[target_name], poly_predictions)\n",
    "print(poly_rmse, poly_mape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5.2 Optional: Run this cell to train the decision tree regression model\n",
    "This cell trains a decision tree regressor using StandardScaler and DecisionTreeRegressor. Decision trees are useful for capturing non-linear patterns without requiring explicit feature engineering.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7294356056931324 0.2586967538598937\n"
     ]
    }
   ],
   "source": [
    "tree_reg = make_pipeline(StandardScaler(), DecisionTreeRegressor(random_state=RAND_SEED))\n",
    "tree_reg.fit(train_df[feature_names], train_df[target_name])\n",
    "tree_predictions = tree_reg.predict(test_df[feature_names])\n",
    "tree_rmse = root_mean_squared_error(test_df[target_name], tree_predictions)\n",
    "tree_mape = mean_absolute_percentage_error(test_df[target_name], tree_predictions)\n",
    "print(tree_rmse, tree_mape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5.3 Optional: Run this cell to train the random forest regression model\n",
    "This cell trains a random forest model, which aggregates multiple decision trees to improve robustness and accuracy. It uses StandardScaler along with RandomForestRegressor to capture ensemble learning benefits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5130851195379558 0.19284714577205617\n"
     ]
    }
   ],
   "source": [
    "forest_reg = make_pipeline(StandardScaler(), RandomForestRegressor(random_state=RAND_SEED))\n",
    "forest_reg.fit(train_df[feature_names], train_df[target_name])\n",
    "forest_predictions = forest_reg.predict(test_df[feature_names])\n",
    "forest_rmse = root_mean_squared_error(test_df[target_name], forest_predictions)\n",
    "forest_mape = mean_absolute_percentage_error(test_df[target_name], forest_predictions)\n",
    "print(forest_rmse, forest_mape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5.4 Optional: Run this cell to train the neural network regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5447239003987849 0.2864130701376177\n"
     ]
    }
   ],
   "source": [
    "mlp_reg = make_pipeline(StandardScaler(), \n",
    "                        MLPRegressor(hidden_layer_sizes=(128,64,32), activation='relu', solver='adam', random_state=RAND_SEED))\n",
    "mlp_reg.fit(train_df[feature_names], train_df[target_name])\n",
    "test_predictions = mlp_reg.predict(test_df[feature_names])\n",
    "mlp_rmse = root_mean_squared_error(test_df[target_name], test_predictions)\n",
    "mlp_mape = mean_absolute_percentage_error(test_df[target_name], test_predictions)\n",
    "print(mlp_rmse, poly_mape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Wrapping the Model with VerifIA\n",
    "\n",
    "VerifIA requires that the model is wrapped in a standardized model wrapper. Here, we use the `build_from_model_card` function to create a `SKLearnModel` instance that encapsulates essential metadata (such as model name, version, type, feature names, target name, and storage directory) along with the trained model. \n",
    "\n",
    "**Note:** You can swap out the pipeline (e.g., use tree, forest, or mlp regressors) by adjusting the wrapper configuration and replacing the wrapped model object.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_wrapper:SKLearnModel = build_from_model_card({\n",
    "    \"name\": \"CHPrice_skl_poly_regressor\", # instead of \"poly\", you should put \"tree\", \"forest\", \"mlp\"\n",
    "    \"version\": \"1\",\n",
    "    \"type\": \"regression\",\n",
    "    \"description\": \"model predicts the prices of california houses\",\n",
    "    \"framework\": \"sklearn\",\n",
    "    \"feature_names\": feature_names,\n",
    "    \"target_name\": target_name,\n",
    "    \"local_dirpath\": MODELS_DIRPATH\n",
    "}).wrap_model(poly_reg) # instead of poly_reg, you should put tree_reg, forest_reg, mlp_reg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. Creating the Domain Configuration\n",
    "\n",
    "We present two options for creating the domain configuration:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Option A – Manual Domain Dictionary**\n",
    "\n",
    "You can manually create a simple domain dictionary. In this example, a dictionary is built where each variable is defined based on the features from the California housing dataframe. A sample constraint (e.g., a ratio between average bedrooms and rooms) and a rule (R1) are included. You can further customize and extend this dictionary with additional rules as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "domain_cfg_dict = {\n",
    "    \"variables\":{\n",
    "        col: {\n",
    "            \"type\": \"INT\" if (is_int := (housing_df[col] == housing_df[col].round()).all()) else \"FLOAT\",\n",
    "            \"range\": (housing_df[col].astype(int) if is_int else housing_df[col]).agg(['min', 'max']).tolist()\n",
    "        }\n",
    "        for col in housing_df.columns\n",
    "    },\n",
    "    \"constraints\":{\n",
    "        \"C1\": {\n",
    "                \"description\":\"\", \n",
    "                \"formula\": \"AveBedrms/AveRooms > 0.5\"\n",
    "            }\n",
    "    },\n",
    "    \"rules\":{\n",
    "        \"R1\": {\n",
    "               \"description\": \"\",\n",
    "               \"premises\": {\"AveRooms\":\"inc\", \"AveBedrms\":\"inc\", \"HouseAge\": \"dec\"},\n",
    "               \"conclusion\": {\"MedHouseVal\":\"inc\"}\n",
    "            }\n",
    "    }\n",
    "}\n",
    "domain_cfg_dict[\"variables\"]['MedHouseVal']['insignificant_variation'] = 0.15 # expect 15% of error as acceptable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Option B – AI-Powered Domain Generation**\n",
    "Alternatively, you can leverage VerifIA’s `DomainGenFlow` to generate a domain dictionary automatically. By providing the dataframe and a description (here, the dataset’s description from `housing.DESCR`), the tool generates a domain configuration using AI. Additionally, you can customize the GPT configuration for domain dictionary generation by setting the environment variables `VERIFIA_GPT_NAME` and `VERIFIA_GPT_TEMPERATURE`. The defaults for these variables are: `VERIFIA_GPT_NAME` is set to \"gpt-4o-mini\" and `VERIFIA_GPT_TEMPERATURE` is set to 0. You can include them in a `.env` file along with your `OPEN_API_KEY` if you want to change the default options."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Setup OpenAI and LangSmith Keys**\n",
    "\n",
    "Note that LangSmith is not needed, but it is helpful. If you do want to use LangSmith, after you sign up at the link above, make sure to set your environment variables to start logging traces.\n",
    "\n",
    "Accessing the OpenAI API requires an API key, which you can get by creating an account. Once you have a key you'll want to set it as an environment variable by running:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = 'true'\n",
    "os.environ[\"LANGCHAIN_ENDPOINT\"] = 'https://api.smith.langchain.com'\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = getpass.getpass(prompt='Your LANGCHAIN_API_KEY? ')\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(prompt='Your OPENAI_API_KEY? ')\n",
    "os.environ[\"USER_AGENT\"] = 'my_agent'\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = 'VERIFIA_TEST'\n",
    "os.environ[\"VERIFIA_GPT_NAME\"] = 'gpt-4.1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'DomainGenFlow' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m domain_genflow \u001b[38;5;241m=\u001b[39m \u001b[43mDomainGenFlow\u001b[49m()\n\u001b[0;32m      2\u001b[0m domain_genflow\u001b[38;5;241m.\u001b[39mload_ctx(dataframe\u001b[38;5;241m=\u001b[39mhousing_df, \n\u001b[0;32m      3\u001b[0m                         db_str_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m(housing\u001b[38;5;241m.\u001b[39mDESCR),\n\u001b[0;32m      4\u001b[0m                         model_card\u001b[38;5;241m=\u001b[39mmodel_wrapper\u001b[38;5;241m.\u001b[39mmodel_card\u001b[38;5;241m.\u001b[39mto_dict())\n\u001b[0;32m      5\u001b[0m domain_cfg_dict \u001b[38;5;241m=\u001b[39m domain_genflow\u001b[38;5;241m.\u001b[39mrun(save\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, local_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./domain.yaml\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'DomainGenFlow' is not defined"
     ]
    }
   ],
   "source": [
    "from verifia.generation import DomainGenFlow\n",
    "\n",
    "domain_genflow = DomainGenFlow()\n",
    "domain_genflow.load_ctx(dataframe=housing_df, \n",
    "                        db_str_content=str(housing.DESCR),\n",
    "                        model_card=model_wrapper.model_card.to_dict())\n",
    "domain_cfg_dict = domain_genflow.run(save=True, local_path=\"./domain.yaml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. Instantiating the Rule Consistency Verifier\n",
    "\n",
    "Using the constructed or generated domain configuration, we instantiate a `RuleConsistencyVerifier`. This component is responsible for checking the consistency of the model with respect to the defined rules and constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_verifier = RuleConsistencyVerifier(domain_cfg_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9. Configuring and Running the Verification\n",
    "\n",
    "Next, we connect the verifier to our wrapped model and the test dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<verifia.verification.verifiers.RuleConsistencyVerifier at 0x14a74166150>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_verifier.verify(model_wrapper).on(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we choose to run the verification using a Genetic Algorithm (GA) as the search strategy. We specify parameters such as:\n",
    "- **Population Size:** 50\n",
    "- **Maximum Iterations:** 10\n",
    "- **Original Seed Size:** 100\n",
    "\n",
    "The verifier explores the input space to identify any rule violations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:verifia.verification.searchers:Searcher built with algorithm: RS\n",
      "INFO:verifia.verification.verifiers:Rows removed because they are out of domain: 100 out of 100\n",
      "ERROR:verifia.models.sklearn:Error during predict.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\users\\ahmed\\desktop\\verifia\\github_projects\\verifia\\verifia\\models\\sklearn.py\", line 157, in predict\n",
      "    pred_outs.predictions = self.model.predict(data)\n",
      "                            ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Ahmed\\anaconda3\\envs\\verifia_test\\Lib\\site-packages\\sklearn\\pipeline.py\", line 787, in predict\n",
      "    Xt = transform.transform(Xt)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Ahmed\\anaconda3\\envs\\verifia_test\\Lib\\site-packages\\sklearn\\utils\\_set_output.py\", line 319, in wrapped\n",
      "    data_to_wrap = f(self, X, *args, **kwargs)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Ahmed\\anaconda3\\envs\\verifia_test\\Lib\\site-packages\\sklearn\\preprocessing\\_data.py\", line 1062, in transform\n",
      "    X = validate_data(\n",
      "        ^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Ahmed\\anaconda3\\envs\\verifia_test\\Lib\\site-packages\\sklearn\\utils\\validation.py\", line 2944, in validate_data\n",
      "    out = check_array(X, input_name=\"X\", **check_params)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Ahmed\\anaconda3\\envs\\verifia_test\\Lib\\site-packages\\sklearn\\utils\\validation.py\", line 1130, in check_array\n",
      "    raise ValueError(\n",
      "ValueError: Found array with 0 sample(s) (shape=(0, 8)) while a minimum of 1 is required by StandardScaler.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found array with 0 sample(s) (shape=(0, 8)) while a minimum of 1 is required by StandardScaler.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m result:RulesViolationResult = \u001b[43mmodel_verifier\u001b[49m\u001b[43m.\u001b[49m\u001b[43musing\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mRS\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43m                                            \u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpop_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_iters\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m                                                 \u001b[49m\u001b[43morig_seed_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\users\\ahmed\\desktop\\verifia\\github_projects\\verifia\\verifia\\verification\\verifiers.py:375\u001b[39m, in \u001b[36mRuleConsistencyVerifier.run\u001b[39m\u001b[34m(self, pop_size, max_iters, orig_seed_ratio, orig_seed_size, persistance)\u001b[39m\n\u001b[32m    370\u001b[39m run_result: RulesViolationRun = \u001b[38;5;28mself\u001b[39m.result.create_new_run(\n\u001b[32m    371\u001b[39m     \u001b[38;5;28mself\u001b[39m.search_algo, \u001b[38;5;28mself\u001b[39m.search_params, pop_size, max_iters\n\u001b[32m    372\u001b[39m )\n\u001b[32m    374\u001b[39m orig_seed_ds = \u001b[38;5;28mself\u001b[39m.dataset.sample(orig_seed_size, orig_seed_ratio)\n\u001b[32m--> \u001b[39m\u001b[32m375\u001b[39m X_filtered, orig_stats = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_filter_inputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43morig_seed_ds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    376\u001b[39m run_result.set_orig_stats(orig_stats)\n\u001b[32m    378\u001b[39m \u001b[38;5;66;03m# Return early if no valid data remains.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\users\\ahmed\\desktop\\verifia\\github_projects\\verifia\\verifia\\verification\\verifiers.py:110\u001b[39m, in \u001b[36mRuleConsistencyVerifier._filter_inputs\u001b[39m\u001b[34m(self, orig_input_seed)\u001b[39m\n\u001b[32m    107\u001b[39m logger.info(\u001b[33m\"\u001b[39m\u001b[33mRows removed because they are out of domain: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m out of \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m, orig_stats.n_ood, original_row_count)\n\u001b[32m    109\u001b[39m features_filtered = filtered_df[\u001b[38;5;28mself\u001b[39m.model.feature_names].to_numpy()\n\u001b[32m--> \u001b[39m\u001b[32m110\u001b[39m outs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures_filtered\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.model.is_regression:\n\u001b[32m    112\u001b[39m     tolerance = \u001b[38;5;28mself\u001b[39m.domain.find_var(\u001b[38;5;28mself\u001b[39m.model.target_name).epsilon\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\users\\ahmed\\desktop\\verifia\\github_projects\\verifia\\verifia\\models\\sklearn.py:157\u001b[39m, in \u001b[36mSKLearnModel.predict\u001b[39m\u001b[34m(self, data)\u001b[39m\n\u001b[32m    155\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    156\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.is_regression:\n\u001b[32m--> \u001b[39m\u001b[32m157\u001b[39m         pred_outs.predictions = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    158\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    159\u001b[39m         pred_outs.predictions = \u001b[38;5;28mself\u001b[39m.model.predict(data)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\verifia_test\\Lib\\site-packages\\sklearn\\pipeline.py:787\u001b[39m, in \u001b[36mPipeline.predict\u001b[39m\u001b[34m(self, X, **params)\u001b[39m\n\u001b[32m    785\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _routing_enabled():\n\u001b[32m    786\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m _, name, transform \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._iter(with_final=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m--> \u001b[39m\u001b[32m787\u001b[39m         Xt = \u001b[43mtransform\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    788\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.steps[-\u001b[32m1\u001b[39m][\u001b[32m1\u001b[39m].predict(Xt, **params)\n\u001b[32m    790\u001b[39m \u001b[38;5;66;03m# metadata routing enabled\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\verifia_test\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:319\u001b[39m, in \u001b[36m_wrap_method_output.<locals>.wrapped\u001b[39m\u001b[34m(self, X, *args, **kwargs)\u001b[39m\n\u001b[32m    317\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[32m    318\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m319\u001b[39m     data_to_wrap = \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    320\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m    321\u001b[39m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[32m    322\u001b[39m         return_tuple = (\n\u001b[32m    323\u001b[39m             _wrap_data_with_container(method, data_to_wrap[\u001b[32m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[32m    324\u001b[39m             *data_to_wrap[\u001b[32m1\u001b[39m:],\n\u001b[32m    325\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\verifia_test\\Lib\\site-packages\\sklearn\\preprocessing\\_data.py:1062\u001b[39m, in \u001b[36mStandardScaler.transform\u001b[39m\u001b[34m(self, X, copy)\u001b[39m\n\u001b[32m   1059\u001b[39m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[32m   1061\u001b[39m copy = copy \u001b[38;5;28;01mif\u001b[39;00m copy \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.copy\n\u001b[32m-> \u001b[39m\u001b[32m1062\u001b[39m X = \u001b[43mvalidate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1063\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1064\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1065\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreset\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1066\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcsr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1067\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1068\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mFLOAT_DTYPES\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1069\u001b[39m \u001b[43m    \u001b[49m\u001b[43mforce_writeable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1070\u001b[39m \u001b[43m    \u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mallow-nan\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1071\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1073\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m sparse.issparse(X):\n\u001b[32m   1074\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.with_mean:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\verifia_test\\Lib\\site-packages\\sklearn\\utils\\validation.py:2944\u001b[39m, in \u001b[36mvalidate_data\u001b[39m\u001b[34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[39m\n\u001b[32m   2942\u001b[39m         out = X, y\n\u001b[32m   2943\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[32m-> \u001b[39m\u001b[32m2944\u001b[39m     out = \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mX\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2945\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n\u001b[32m   2946\u001b[39m     out = _check_y(y, **check_params)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\verifia_test\\Lib\\site-packages\\sklearn\\utils\\validation.py:1130\u001b[39m, in \u001b[36mcheck_array\u001b[39m\u001b[34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[39m\n\u001b[32m   1128\u001b[39m     n_samples = _num_samples(array)\n\u001b[32m   1129\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m n_samples < ensure_min_samples:\n\u001b[32m-> \u001b[39m\u001b[32m1130\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1131\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mFound array with \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[33m sample(s) (shape=\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m) while a\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1132\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33m minimum of \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[33m is required\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1133\u001b[39m             % (n_samples, array.shape, ensure_min_samples, context)\n\u001b[32m   1134\u001b[39m         )\n\u001b[32m   1136\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ensure_min_features > \u001b[32m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m array.ndim == \u001b[32m2\u001b[39m:\n\u001b[32m   1137\u001b[39m     n_features = array.shape[\u001b[32m1\u001b[39m]\n",
      "\u001b[31mValueError\u001b[39m: Found array with 0 sample(s) (shape=(0, 8)) while a minimum of 1 is required by StandardScaler."
     ]
    }
   ],
   "source": [
    "result:RulesViolationResult = model_verifier.using(\"RS\")\\\n",
    "                                            .run(pop_size=50, max_iters=10, \n",
    "                                                 orig_seed_size=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10. Saving the Results and Model Artifacts\n",
    "\n",
    "Finally, the verification results are saved as an HTML report, which provides a comprehensive summary of the rule evaluations and any detected inconsistencies. Additionally, the model and its model card are saved for future reference and further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.save_as_html(\"../reports/CHPrice_skl_poly_regressor.html\")  \n",
    "# \"poly\" is used for the polynomial model, \n",
    "# ensure that the report's filename includes a short identifier reflecting the specific model used. \n",
    "# For instance, use \"tree\" for a decision tree model, \"forest\" for a random forest model, and \"mlp\" for a multi-layer perceptron. \n",
    "# This naming convention makes it easier to quickly identify which model generated the report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_wrapper.save_model()\n",
    "model_wrapper.save_model_card(\"../models/CHPrice_skl_poly_regressor.yaml\")\n",
    "# \"poly\" is used for the polynomial model, \n",
    "# ensure that the model card's filename includes a short identifier reflecting the specific model used. \n",
    "# For instance, use \"tree\" for a decision tree model, \"forest\" for a random forest model, and \"mlp\" for a multi-layer perceptron. \n",
    "# This naming convention makes it easier to quickly identify which model is described by the card."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
